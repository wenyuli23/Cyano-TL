{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "\n",
    "import shap\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def plot_predictions(y_test, y_pred, y_pred_before, x_test, target, output_file='scatter_plot.png'):\n",
    "    \"\"\"\n",
    "    Plots predicted vs actual values with different markers for \"after TL\" and \"before TL\"\n",
    "    and color-coded by unique products.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test: pd.Series or np.ndarray, actual target values\n",
    "    - y_pred: pd.Series or np.ndarray, predicted values from the model after transfer learning\n",
    "    - y_pred_before: pd.Series or np.ndarray, predicted values from the model before transfer learning\n",
    "    - x_test: pd.DataFrame, feature set including the 'product_old' column\n",
    "    - target: str, target variable name used in title and output\n",
    "    - output_file: str, file name to save the plot (default: 'scatter_plot.png')\n",
    "    \"\"\"\n",
    "\n",
    "    unique_products = x_test['product'].str.replace(' ', '').unique()\n",
    "    cmap = plt.colormaps['tab10']\n",
    "    colors = [cmap(i / len(unique_products)) for i in range(len(unique_products))]\n",
    "    color_dict = {product: colors[i] for i, product in enumerate(unique_products)}\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for product in unique_products:\n",
    "        mask = x_test['product'] == product\n",
    "        plt.scatter(y_test[mask], y_pred[mask], c=[color_dict[product]], label=f'{product}', alpha=0.5, marker='o')\n",
    "\n",
    "    for product in unique_products:\n",
    "        mask = x_test['product'] == product\n",
    "        plt.scatter(y_test[mask], y_pred_before[mask], c=[color_dict[product]], alpha=0.5, marker='x')\n",
    "\n",
    "    plt.plot([min(y_test.values), max(y_test.values)], [min(y_test.values), max(y_test.values)], 'k--')\n",
    "\n",
    "    circle_patch = Line2D([], [], marker='o', color='w', markerfacecolor='gray', markersize=8, label='after TL')\n",
    "    cross_patch = Line2D([], [], marker='x', color='w', markeredgecolor='gray', markersize=8, label='before TL')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "    #handles.extend([circle_patch, cross_patch])\n",
    "    #labels.extend(['after TL', 'before TL'])\n",
    "\n",
    "    plt.legend(handles, labels, loc='best', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{target} predictions for all products')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../figures/{output_file}', dpi=300)\n",
    "\n",
    "def apply_imputation(df, rules):\n",
    "    for _, row in rules.iterrows():\n",
    "        feature = row.iloc[0]\n",
    "        method = row.iloc[1]\n",
    "        #print(f\"Imputing {feature} with {method} method.\")\n",
    "        if feature in df.columns:\n",
    "            if method == 'average':\n",
    "                df[feature] = df[feature].astype('float64')\n",
    "                mean_value = df[feature].mean()\n",
    "                df.fillna({feature: mean_value}, inplace=True)\n",
    "            elif method == 'mode':\n",
    "                mode_value = df[feature].mode()[0]\n",
    "                df.fillna({feature: mode_value}, inplace=True)\n",
    "            elif method == 0:\n",
    "                df.fillna({feature: 0}, inplace=True)\n",
    "            elif method == 'na as a category':\n",
    "                df[feature] = df[feature].astype('category')\n",
    "                if 'missing' not in df[feature].cat.categories:\n",
    "                    df[feature] = df[feature].cat.add_categories('missing')\n",
    "                df.fillna({feature: 'missing'}, inplace=True)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    encoder = OrdinalEncoder()\n",
    "    cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    df_enc = df.copy()\n",
    "    df_enc[cols] = df_enc[cols].astype(str)\n",
    "    df_enc[cols] = encoder.fit_transform(df_enc[cols])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scale = pd.DataFrame(scaler.fit_transform(df_enc), columns=df_enc.columns)\n",
    "\n",
    "    return df_scale\n",
    "\n",
    "def plot_top10_shap_values(model, model_before, features_to_include, x_test, target):\n",
    "    \"\"\"\n",
    "    Plots the top 10 features with the highest SHAP values before and after transfer learning (TL).\n",
    "    \n",
    "    This function uses SHAP (SHapley Additive exPlanations) to interpret the contributions of features\n",
    "    in two models. It highlights how the importance of the selected features changes between the models.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model after transfer learning.\n",
    "        model_before: Trained model before transfer learning.\n",
    "        features_to_include: List of feature names to consider for SHAP value computation.\n",
    "        x_test: DataFrame containing the test data used for SHAP value computation.\n",
    "        target: Name of the target variable for the model (used in plot titles and file naming).\n",
    "    \n",
    "    Steps:\n",
    "        1. Compute SHAP values for both models using the provided test data.\n",
    "        2. Filter SHAP values and test data to include only the specified features.\n",
    "        3. Calculate the mean absolute SHAP values for both models.\n",
    "        4. Identify the top 10 features with the highest combined SHAP values.\n",
    "        5. Create a scatter plot showing SHAP values for each model:\n",
    "            - Use circles for the model after TL.\n",
    "            - Use crosses for the model before TL.\n",
    "        6. Use color mapping to represent the actual feature values.\n",
    "        7. Save the plot as a PNG file in the `../figures/` directory.\n",
    "\n",
    "    Output:\n",
    "        - A scatter plot visualizing the top 10 feature contributions before and after transfer learning,\n",
    "          with a color bar indicating the actual feature values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    explainer2 = shap.TreeExplainer(model_before)\n",
    "    shap_values = explainer.shap_values(x_test)\n",
    "    shap_values2 = explainer2.shap_values(x_test)\n",
    "\n",
    "    features_to_include_indices = [x_test.columns.get_loc(feature) for feature in features_to_include]\n",
    "\n",
    "    filtered_shap_values = shap_values[:, features_to_include_indices]\n",
    "    filtered_shap_values2 = shap_values2[:, features_to_include_indices]\n",
    "\n",
    "    filtered_features = x_test[features_to_include]\n",
    "\n",
    "    mean_abs_shap_values1 = np.mean(np.abs(filtered_shap_values), axis=0)\n",
    "    mean_abs_shap_values2 = np.mean(np.abs(filtered_shap_values2), axis=0)\n",
    "\n",
    "    # Get the top 10 features by average SHAP value for both before TL and after TL\n",
    "    top_10_indices = np.argsort(mean_abs_shap_values1 + mean_abs_shap_values2)[-10:]\n",
    "    top_10_features = np.array(features_to_include)[top_10_indices]\n",
    "\n",
    "    # Filter SHAP values and feature data for top 10 features\n",
    "    shap_values1_top10 = filtered_shap_values[:, top_10_indices]\n",
    "    shap_values2_top10 = filtered_shap_values2[:, top_10_indices]\n",
    "    X_test1_top10 = filtered_features.iloc[:, top_10_indices]\n",
    "    X_test2_top10 = filtered_features.iloc[:, top_10_indices]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i, feature in enumerate(top_10_features):\n",
    "        # Normalize feature values for color mapping\n",
    "        norm = plt.Normalize(vmin=X_test1_top10[feature].min(), vmax=X_test1_top10[feature].max())\n",
    "        colors1 = plt.cm.coolwarm(norm(X_test1_top10[feature]))\n",
    "        colors2 = plt.cm.coolwarm(norm(X_test2_top10[feature]))\n",
    "        \n",
    "        # Plot SHAP values for after TL with circles\n",
    "        ax.scatter(shap_values1_top10[:, i], [i + 0.2] * len(shap_values1_top10), alpha=0.5, label='after TL' if i == 0 else \"\", marker='o', c=colors1)\n",
    "        # Plot SHAP values for before TL with crosses\n",
    "        ax.scatter(shap_values2_top10[:, i], [i - 0.2] * len(shap_values2_top10), alpha=0.5, label='before TL' if i == 0 else \"\", marker='x', c=colors2)\n",
    "\n",
    "        ax.hlines(y=i, xmin=min(shap_values1_top10[:, i].min(), shap_values2_top10[:, i].min()), xmax=max(shap_values1_top10[:, i].max(), shap_values2_top10[:, i].max()), colors='grey', linestyles='dashed', alpha=0.3)\n",
    "        \n",
    "    ax.axvline(x=0, color='grey', linestyle='--')\n",
    "    ax.set_yticks(np.arange(len(top_10_features)))\n",
    "    ax.set_yticklabels(top_10_features)\n",
    "    ax.set_xlabel(\"SHAP value (impact on model output)\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='grey', markersize=10, label='after TL'),\n",
    "        Line2D([0], [0], marker='x', color='w', markeredgecolor='grey', markersize=10, label='before TL')\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc='best')\n",
    "    \n",
    "    color_bar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='coolwarm'), ax=ax, label='Actual feature value', aspect=30, pad=0.01, shrink=0.6)\n",
    "    color_bar.outline.set_visible(False)  # Remove color bar frame\n",
    "    plt.title(f'Feature importance for {target} prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../figures/{target}_shap.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filename = \"../data/6803_v2.xlsx\"\n",
    "source_filename2 = \"../data/7942_v2.xlsx\"\n",
    "target_filename = \"../data/2973_v2.xlsx\"\n",
    "\n",
    "df_source1 = pd.read_excel(source_filename, usecols=lambda x: x != 'Unnamed: 0', na_values=['na', ''])\n",
    "df_source2 = pd.read_excel(source_filename2, usecols=lambda x: x != 'Unnamed: 0', na_values=['na', ''])\n",
    "df_source = pd.concat([df_source1, df_source2], ignore_index=True)\n",
    "df_target = pd.read_excel(target_filename, usecols=lambda x: x != 'Unnamed: 0', na_values=['na', ''])\n",
    "\n",
    "imputation_rules = pd.read_excel('../data/impute.xlsx', header=None)\n",
    "\n",
    "apply_imputation(df_source, imputation_rules)\n",
    "apply_imputation(df_target, imputation_rules)\n",
    "\n",
    "output_features = ['OD', 'growth_rate', 'product_titer', 'production_rate']\n",
    "input_features = df_source.columns.difference(output_features + ['paper'])\n",
    "\n",
    "df_source['paper'] = df_source['paper'].astype(int)\n",
    "df_target['paper'] = df_target['paper'].astype(int)\n",
    "\n",
    "features_to_include = ['temperature', 'light_intensity', 'light_dark_ratio', 'reactor_volume',\n",
    "                    'reaction_volume', 'initial_OD', 'cultivation_time', 'no3_conc',\n",
    "                    'phosphate_conc', 'co2_perc', 'air_bubbling', 'bubbling_speed (ml/min)',\n",
    "                    'hco3_conc', 'nacl_conc', 'medium_pH', 'substrate2_conc(mg/L)',\n",
    "                    'substrate2_enthalpyFormation', 'genetic_wt', 'inducerConc',\n",
    "                    'genetic_mutation', 'genetic_knockoutSteps', 'genetic_enzymaticSteps',\n",
    "                    'genetic_heterologousSteps', 'pathwayOptimized', 'numberC', 'numberH',\n",
    "                    'numberN', 'numberO', 'product_enthalpyFormation', 'product permeability']\n",
    "\n",
    "num_test_paper = 3\n",
    "paper_index = df_target[\"paper\"].unique()\n",
    "combinations = list(itertools.combinations(paper_index, num_test_paper))\n",
    "\n",
    "results = pd.DataFrame(columns=[\"combo\", \"target_feature\", \"before_rmse\", \"before_r2\", \"before_mape\", \"after_rmse\", \"after_r2\", \"after_mape\", \"n_worst_trees\"])\n",
    "\n",
    "for test_paper_list in tqdm(combinations, desc=\"Processing combinations\"):\n",
    "    for target in output_features:\n",
    "        df_test = df_target[df_target['paper'].isin(test_paper_list)]\n",
    "        y_test = df_test[target]\n",
    "        df_train = df_target[~df_target['paper'].isin(test_paper_list)]\n",
    "        y_train = df_train[target]\n",
    "        X_source_processed = preprocess_data(df_source[input_features])\n",
    "        X_train_processed = preprocess_data(df_train[input_features])\n",
    "        X_test_processed = preprocess_data(df_test[input_features])\n",
    "\n",
    "        rf_regressor = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        param_grid = {\n",
    "            'bootstrap': [True],\n",
    "            'max_depth': [10, None],\n",
    "            'min_samples_leaf': [1],\n",
    "            'min_samples_split': [2],\n",
    "            'n_estimators': [100, 200],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf_regressor,\n",
    "            param_grid=param_grid,\n",
    "            scoring='r2',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train_processed, y_train)\n",
    "        best_rf = grid_search.best_estimator_\n",
    "        best_rf_before = copy.deepcopy(best_rf)\n",
    "        #print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "        y_pred_before = best_rf.predict(X_test_processed)\n",
    "        before_rmse = mean_squared_error(y_test, y_pred_before)\n",
    "        before_r2 = r2_score(y_test, y_pred_before)\n",
    "        before_mape = mean_absolute_percentage_error(y_test, y_pred_before)\n",
    "\n",
    "        # Transfer\n",
    "        tree_r2_scores = []\n",
    "        for tree in best_rf.estimators_:\n",
    "            y_pred_tree = tree.predict(X_test_processed.values)\n",
    "            score = r2_score(y_test, y_pred_tree)\n",
    "            tree_r2_scores.append(score)\n",
    "        # Sort the trees ascending by r2\n",
    "        sorted_tree_indices = np.argsort(tree_r2_scores)\n",
    "        \n",
    "        n_worst_trees = [10, 15, 20, 25, 30]\n",
    "        for n_tree in n_worst_trees:\n",
    "            for idx, tree_idx in zip(range(n_tree), sorted_tree_indices[:n_tree]):\n",
    "                tree = best_rf.estimators_[tree_idx]\n",
    "                new_X_train = pd.concat([X_source_processed, X_train_processed], axis = 0)\n",
    "                new_y_train = pd.concat([df_source[target], y_train], axis = 0)\n",
    "                tree.fit(new_X_train, new_y_train)\n",
    "\n",
    "            y_pred_after = best_rf.predict(X_test_processed)\n",
    "            new_r2 = r2_score(y_test, y_pred_after)\n",
    "            new_rmse = mean_squared_error(y_test, y_pred_after)\n",
    "            new_mape = mean_absolute_percentage_error(y_test, y_pred_after)\n",
    "            result_list = [test_paper_list, target, before_rmse, before_r2, before_mape, new_rmse, new_r2, new_mape, n_tree]\n",
    "\n",
    "            results.loc[len(results)] = result_list\n",
    "        \n",
    "        #plot_predictions(y_test, best_y_pred, y_pred_before, df_test[input_features], target, output_file=f'{target}_scatter_plot.png')\n",
    "\n",
    "        #plot_top10_shap_values(best_rf, best_rf_before, features_to_include, X_test_processed, target)\n",
    "results.to_csv(f'../data/results_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
